{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWOaO3d7M5n0",
    "outputId": "3d29ae66-7a7f-4340-e2b6-74771645506b"
   },
   "outputs": [],
   "source": [
    "!unzip /content/Planets_Moons_Data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztUox8JjUh62"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQym6szo-394"
   },
   "source": [
    "##Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slk9Z40hJ98G",
    "outputId": "1ac483f3-fa0e-4ad4-83e6-8801a423d580"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=1.0/255.0,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    dtype=None)\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=1.0/255.0,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    dtype=None)\n",
    "train_generator = train_datagen.flow_from_directory(\"Planets_Moons_Data/Planets and Moons\",target_size=(256, 256),\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    interpolation=\"lanczos\",\n",
    "                                                    subset=\"training\")\n",
    "test_generator = test_datagen.flow_from_directory(\"Planets_Moons_Data/Planets and Moons\",target_size=(256, 256),\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    interpolation=\"lanczos\",\n",
    "                                                    subset=\"validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, SpatialDropout2D, MaxPooling2D\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_transfer_model():\n",
    "    # Load the ResNet101 model without the top (fully connected) layers\n",
    "    base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "    # Freeze the pre-trained layers so they are not trainable\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #custom top layers\n",
    "    x = base_model.output\n",
    "    x = SpatialDropout2D(0.2)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', data_format=None)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    num_classes = 11\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model with your preferred optimizer and loss function\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_transfer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "model.fit(train_generator, epochs=epochs)\n",
    "\n",
    "# Evaluate the model using the test_generator\n",
    "loss, accuracy = model.evaluate(train_generator)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Get predictions on the test data\n",
    "num_test_samples = len(test_generator.filenames)\n",
    "num_classes = len(test_generator.class_indices)\n",
    "test_steps_per_epoch = np.ceil(num_test_samples / batch_size)\n",
    "predictions = model.predict(test_generator, steps=test_steps_per_epoch)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "true_labels = train_generator.classes\n",
    "confusion_mtx = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test data\n",
    "num_test_samples = len(train_generator.filenames)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "test_steps_per_epoch = np.ceil(num_test_samples / batch_size)\n",
    "predictions = model.predict(train_generator, steps=test_steps_per_epoch)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "true_labels = train_generator.classes\n",
    "confusion_mtx = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TN = confusion_mtx.sum() - confusion_mtx.sum(axis=0) - confusion_mtx.sum(axis=1) + np.diag(confusion_mtx)\n",
    "FP = confusion_mtx.sum(axis=0) - np.diag(confusion_mtx)\n",
    "TP = np.diag(confusion_mtx)\n",
    "FN = confusion_mtx.sum(axis=1) - np.diag(confusion_mtx)\n",
    "\n",
    "# Calculate percentages\n",
    "total_samples = TN + FP + TP + FN\n",
    "TN_percentage = (TN / total_samples) * 100.0\n",
    "FP_percentage = (FP / total_samples) * 100.0\n",
    "TP_percentage = (TP / total_samples) * 100.0\n",
    "FN_percentage = (FN / total_samples) * 100.0\n",
    "\n",
    "# Display TN, FP, TP, FN in percentage\n",
    "print(f\"True Negatives (TN) Percentage: {TN_percentage.sum():.2f}%\")\n",
    "print(f\"False Positives (FP) Percentage: {FP_percentage.sum():.2f}%\")\n",
    "print(f\"True Positives (TP) Percentage: {TP_percentage.sum():.2f}%\")\n",
    "print(f\"False Negatives (FN) Percentage: {FN_percentage.sum():.2f}%\")\n",
    "\n",
    "# Plot the confusion matrix with percentages\n",
    "percentage_confusion_mtx = np.array([[TN_percentage.sum(), FP_percentage.sum()],\n",
    "                                    [FN_percentage.sum(), TP_percentage.sum()]])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(percentage_confusion_mtx, annot=True, cmap=\"Blues\", fmt=\".2f\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQfN3sSV2B8e"
   },
   "outputs": [],
   "source": [
    "def our_model():\n",
    "  inp = Input(shape = (256,256,3))\n",
    "\n",
    "  x = Conv2D(128, (2,2), strides=(2,2), padding='same', activation='ReLU', use_bias=True)(inp)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = SpatialDropout2D(0.2)(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "  x = Conv2D(256, (2,2), strides=(2,2), padding='same', activation='ReLU', use_bias=True)(x)\n",
    "  x = SpatialDropout2D(0.2)(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "  x = Conv2D(512, (2,2), strides=(2,2), padding='same', activation='ReLU', use_bias=True)(x)\n",
    "  x = SpatialDropout2D(0.2)(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(128, activation='ReLU')(x)\n",
    "  x = Dense(11, activation='softmax')(x)\n",
    "\n",
    "  model = Model(inputs=inp, outputs= x)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ug-8awarRgT2"
   },
   "outputs": [],
   "source": [
    "def func(name_model):\n",
    "\n",
    "    print('#####~Model => {} '.format(name_model))\n",
    "\n",
    "    model = our_model()\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    my_callbacks  =  [keras.callbacks.ModelCheckpoint(\"/content/model/model_{epoch}.h5\")]\n",
    "    \n",
    "    history = model.fit(train_generator,\n",
    "                        validation_data=test_generator,\n",
    "                        epochs=48,\n",
    "                        callbacks=my_callbacks,\n",
    "                        verbose=1,\n",
    "                        batch_size=128,)\n",
    "    # Plotting Accuracy, val_accuracy, loss, val_loss\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict Data Test\n",
    "    pred = model.predict(test_generator)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "    \n",
    "    print('\\033[01m              Classification_report \\033[0m')\n",
    "    \n",
    "    print('\\033[01m              Results \\033[0m')\n",
    "    # Results\n",
    "    results = model.evaluate(test_generator, verbose=0)\n",
    "    print(\"    Test Loss:\\033[31m \\033[01m {:.5f} \\033[30m \\033[0m\".format(results[0]))\n",
    "    print(\"Test Accuracy:\\033[32m \\033[01m {:.2f}% \\033[30m \\033[0m\".format(results[1] * 100))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbYGWaUb6Gkp"
   },
   "outputs": [],
   "source": [
    "def func(pre,name_model):\n",
    "    print('#####~Model => {} '.format(name_model))\n",
    "    pre_model = name_model(input_shape=(256,256, 3),\n",
    "                   include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   pooling='avg')\n",
    "    pre_model.trainable = False\n",
    "    inputs = pre_model.input\n",
    "    x = Dense(64, activation='relu')(pre_model.output)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(11, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "    my_callbacks  = [EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              mode='auto')]\n",
    "    \n",
    "    history = model.fit(train_generator,validation_data=test_generator,epochs=48,callbacks=my_callbacks,verbose=0)\n",
    "    # Plotting Accuracy, val_accuracy, loss, val_loss\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict Data Test\n",
    "    pred = model.predict(test_generator)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "    \n",
    "    print('\\033[01m              Classification_report \\033[0m')\n",
    "    \n",
    "    print('\\033[01m              Results \\033[0m')\n",
    "    # Results\n",
    "    results = model.evaluate(test_generator, verbose=0)\n",
    "    print(\"    Test Loss:\\033[31m \\033[01m {:.5f} \\033[30m \\033[0m\".format(results[0]))\n",
    "    print(\"Test Accuracy:\\033[32m \\033[01m {:.2f}% \\033[30m \\033[0m\".format(results[1] * 100))\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Planets_and_Moons_Dataset_AI_in_Space_ðŸŒŒ_ðŸ›°_â˜„_ðŸ”­.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
